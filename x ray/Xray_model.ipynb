{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### libraries\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "Z-R2NpagEPNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngb85KjlkktB"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import os\n",
        "from google.colab import files\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "import numpy as np\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mount to drive\n",
        "--------------------------------"
      ],
      "metadata": {
        "id": "awQPczqLFshR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-KFVKJLHkl_",
        "outputId": "581a7778-d05a-46f5-ae1a-64220fc14900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------\n",
        "#connect to kaggle for dataset\n",
        "----------------------------------------"
      ],
      "metadata": {
        "id": "HA0y0gZOFxut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Kaggle API token to Colab\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "DCI81ONVuCgD",
        "outputId": "5d34cf8a-8c5b-434a-b657-50f9c7fedc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-471ddac3-82f3-4622-8b07-a35950838902\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-471ddac3-82f3-4622-8b07-a35950838902\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mohamedeslammohamed\",\"key\":\"534826d15fd337218e097a5729814682\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This allows Colab to authenticate with Kaggle.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "HuLcXZ4vwUJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset from Kaggle\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db8yEmmCwimz",
        "outputId": "3f5d33fe-00fe-4719-9a2b-054252ce74d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n",
            "Downloading chest-xray-pneumonia.zip to /content\n",
            " 96% 2.21G/2.29G [00:29<00:01, 47.3MB/s]\n",
            "100% 2.29G/2.29G [00:29<00:00, 83.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "!unzip -q chest-xray-pneumonia.zip -d /content/\n"
      ],
      "metadata": {
        "id": "ocB9R2Qvww96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm dataset structure\n",
        "base = \"/content/chest_xray\"\n",
        "\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    for cls in os.listdir(os.path.join(base, split)):\n",
        "        path = os.path.join(base, split, cls)\n",
        "        print(split, cls, len(os.listdir(path)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtF7q-TPw-me",
        "outputId": "e189629d-c587-4610-e7e7-10f0ed442de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train PNEUMONIA 3875\n",
            "train NORMAL 1341\n",
            "val PNEUMONIA 8\n",
            "val NORMAL 8\n",
            "test PNEUMONIA 390\n",
            "test NORMAL 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------\n",
        "# preprocessing for vlidation and test\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "fYmiVeh6F_PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing transforms\n",
        "# Why: DenseNet121 expects 3-channel images normalized to ImageNet statistics and a fixed size (commonly 224×224).\n",
        "IMG_SIZE = 224\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),   # if images are single-channel\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "fO41yoxyxxEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(\"/content/chest_xray/val/NORMAL/NORMAL2-IM-1427-0001.jpeg\").convert('L')  # grayscale\n",
        "t = val_transforms(img)\n",
        "print(t.shape, t.min().item(), t.max().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXdRqBFtzKfA",
        "outputId": "36d3ea2b-3bb3-44e5-a0bc-bdec1563a84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224]) -2.1179039478302 2.465708017349243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------\n",
        "# preprocessing for train\n",
        "---------------------------------------"
      ],
      "metadata": {
        "id": "00ouPOk9GN2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase robustness and reduce overfitting while preserving medically meaningful features.\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0), ratio=(0.9,1.1)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),  # small rotation\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "q8SVL-rLzVam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define datasets\n",
        "train_ds = ImageFolder(os.path.join(base, \"train\"), transform=train_transforms)\n",
        "val_ds   = ImageFolder(os.path.join(base, \"val\"),   transform=val_transforms)\n",
        "test_ds  = ImageFolder(os.path.join(base, \"test\"),  transform=val_transforms)\n",
        "\n",
        "#  Batch size (adjust if out of memory)\n",
        "BATCH_SIZE = 16  # try 8 if T4 runs out of RAM\n",
        "\n",
        "#  Create data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "#  Test one batch\n",
        "imgs, labels = next(iter(train_loader))\n",
        "print(imgs.shape, labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXIyTaK-2C1I",
        "outputId": "e5425bea-0da7-46ec-fc55-626abccc14b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 224, 224]) torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------\n",
        "# upload the data set of the unknown images\n",
        "-----------------------------------------------"
      ],
      "metadata": {
        "id": "MY7x4BltGZjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "# Source path of your existing X-ray data (Standard Colab/Kaggle path)\n",
        "SOURCE_XRAY_ROOT = \"/content/chest_xray\"\n",
        "\n",
        "# Destination path for the new clean 3-class dataset\n",
        "NEW_DATASET_ROOT = \"/content/three_class_dataset\"\n",
        "\n",
        "# Number of 'Unknown' images to generate for each split\n",
        "COUNTS = {\n",
        "    \"train\": 2000,  # Add 5000 unknown images to training\n",
        "    \"test\": 600,    # Add 600 unknown images to testing\n",
        "    \"val\": 50       # Add 50 unknown images to validation\n",
        "}\n",
        "\n",
        "# ---------------------\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"Creates the train/val/test directories for the new dataset.\"\"\"\n",
        "    for split in [\"train\", \"test\", \"val\"]:\n",
        "        for cls in [\"NORMAL\", \"PNEUMONIA\", \"UNKNOWN\"]:\n",
        "            os.makedirs(os.path.join(NEW_DATASET_ROOT, split, cls), exist_ok=True)\n",
        "\n",
        "def copy_xrays():\n",
        "    \"\"\"Copies existing X-ray images to the new dataset structure.\"\"\"\n",
        "    print(\"\\n--- Copying X-Ray Images (Normal & Pneumonia) ---\")\n",
        "\n",
        "    splits = [\"train\", \"test\", \"val\"]\n",
        "    classes = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "\n",
        "    for split in splits:\n",
        "        for cls in classes:\n",
        "            src_dir = os.path.join(SOURCE_XRAY_ROOT, split, cls)\n",
        "            dst_dir = os.path.join(NEW_DATASET_ROOT, split, cls)\n",
        "\n",
        "            if not os.path.exists(src_dir):\n",
        "                print(f\"Warning: Source directory not found: {src_dir}\")\n",
        "                continue\n",
        "\n",
        "            # Copy files\n",
        "            files = os.listdir(src_dir)\n",
        "            print(f\"Copying {len(files)} images from {split}/{cls}...\")\n",
        "            for f in files:\n",
        "                src_file = os.path.join(src_dir, f)\n",
        "                dst_file = os.path.join(dst_dir, f)\n",
        "                if os.path.isfile(src_file):\n",
        "                    shutil.copy(src_file, dst_file)\n",
        "\n",
        "def create_unknown_data():\n",
        "    \"\"\"Downloads CIFAR-100 and distributes it as 'UNKNOWN' class.\"\"\"\n",
        "    print(\"\\n--- Generating 'UNKNOWN' Class from CIFAR-100 ---\")\n",
        "\n",
        "    # Load CIFAR-100 (We use train set for everything to get enough quantity)\n",
        "    cifar_ds = datasets.CIFAR100(root='./data_cifar', train=True, download=True)\n",
        "\n",
        "    total_idx = 0\n",
        "\n",
        "    for split, count in COUNTS.items():\n",
        "        print(f\"Creating {count} unknown images for {split} set...\")\n",
        "        save_dir = os.path.join(NEW_DATASET_ROOT, split, \"UNKNOWN\")\n",
        "\n",
        "        for i in range(count):\n",
        "            # Get image from CIFAR\n",
        "            img, _ = cifar_ds[total_idx]\n",
        "\n",
        "            # Save as JPEG\n",
        "            save_path = os.path.join(save_dir, f\"unknown_{total_idx:05d}.jpg\")\n",
        "            img.save(save_path)\n",
        "\n",
        "            total_idx += 1\n",
        "\n",
        "            # Safety break if we run out of CIFAR images (CIFAR train has 50k)\n",
        "            if total_idx >= len(cifar_ds):\n",
        "                break\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    setup_directories()\n",
        "    copy_xrays()\n",
        "    create_unknown_data()\n",
        "\n",
        "    # Clean up CIFAR download cache\n",
        "    if os.path.exists('./data_cifar'):\n",
        "        shutil.rmtree('./data_cifar')\n",
        "\n",
        "    print(f\"\\nSUCCESS: Dataset ready at {NEW_DATASET_ROOT}\")\n",
        "    print(\"Classes present: NORMAL, PNEUMONIA, UNKNOWN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF7jmFTeUZIF",
        "outputId": "d8ffa1b3-af3a-4493-c8e2-cb07958d35dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Copying X-Ray Images (Normal & Pneumonia) ---\n",
            "Copying 1341 images from train/NORMAL...\n",
            "Copying 3875 images from train/PNEUMONIA...\n",
            "Copying 234 images from test/NORMAL...\n",
            "Copying 390 images from test/PNEUMONIA...\n",
            "Copying 8 images from val/NORMAL...\n",
            "Copying 8 images from val/PNEUMONIA...\n",
            "\n",
            "--- Generating 'UNKNOWN' Class from CIFAR-100 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 34.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 2000 unknown images for train set...\n",
            "Creating 600 unknown images for test set...\n",
            "Creating 50 unknown images for val set...\n",
            "\n",
            "SUCCESS: Dataset ready at /content/three_class_dataset\n",
            "Classes present: NORMAL, PNEUMONIA, UNKNOWN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "DATASET_ROOT = \"/content/three_class_dataset\"\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# --- DATASETS & LOADERS ---\n",
        "\n",
        "def get_data_loaders():\n",
        "    print(\"Initializing Data Loaders...\")\n",
        "\n",
        "    # 1. Define Datasets (ImageFolder automatically finds classes by folder name)\n",
        "    train_ds = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"train\"), transform=train_transforms)\n",
        "    val_ds   = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"val\"), transform=val_transforms)\n",
        "    test_ds  = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"test\"), transform=val_transforms)\n",
        "\n",
        "    # 2. Define Loaders\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 3. Print Summary\n",
        "    class_names = train_ds.classes\n",
        "    print(f\"Classes found: {class_names}\")\n",
        "    print(f\"Training samples: {len(train_ds)}\")\n",
        "    print(f\"Validation samples: {len(val_ds)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_names\n",
        "\n",
        "# --- USAGE ---\n",
        "train_loader, val_loader, test_loader, class_names = get_data_loaders()\n",
        "\n",
        "# Verify a single batch\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"\\nBatch shape: {images.shape}\") # Should be [32, 3, 224, 224]\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q-SYSFiVbej",
        "outputId": "54f8e73d-f138-4ed1-bb76-0b57b343a145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Data Loaders...\n",
            "Classes found: ['NORMAL', 'PNEUMONIA', 'UNKNOWN']\n",
            "Training samples: 7216\n",
            "Validation samples: 66\n",
            "\n",
            "Batch shape: torch.Size([16, 3, 224, 224])\n",
            "Labels shape: torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAIN CLASSIFIER ONLY"
      ],
      "metadata": {
        "id": "Lf8cgu5IGxVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Load pretrained DenseNet121 (ImageNet weights)\n",
        "# This downloads the weights if not already cached\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# 2. Freeze the backbone (Feature Extractor)\n",
        "# We iterate through the 'features' part of DenseNet and turn off gradient calculation.\n",
        "# This ensures we don't destroy the pre-trained patterns during the initial training.\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 3. Replace final classifier layer for 3 classes\n",
        "# Classes: 0: Normal, 1: Pneumonia, 2: Unknown (Non-X-ray)\n",
        "n_features = model.classifier.in_features\n",
        "\n",
        "# We create a new Linear layer. By default, requires_grad is True for new layers.\n",
        "model.classifier = nn.Linear(n_features, 3)\n",
        "\n",
        "# 4. Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 5. Verification: Print total and trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Device used: {device}\")\n",
        "print(\"Model structure modified for 3 classes: [Normal, Pneumonia, Unknown]\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if trainable_params < total_params:\n",
        "    print(\"SUCCESS: Backbone is frozen. Only the classifier will train.\")\n",
        "else:\n",
        "    print(\"WARNING: Backbone appears to be unfrozen.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqByeWNCR42c",
        "outputId": "7f1bc81a-549e-492b-c10b-4355ac7d5eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 173MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cuda\n",
            "Model structure modified for 3 classes: [Normal, Pneumonia, Unknown]\n",
            "------------------------------\n",
            "Total parameters: 6,956,931\n",
            "Trainable parameters: 3,075\n",
            "------------------------------\n",
            "SUCCESS: Backbone is frozen. Only the classifier will train.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RECALCULATING WEIGHTS FOR 3 CLASSES ---\n",
        "\n",
        "# Order matches [Normal, Pneumonia, Unknown] (0, 1, 2)\n",
        "class_weights = torch.tensor([1.78, 0.62, 1.20], dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "print(f\"Using Class Weights: {class_weights}\")\n",
        "\n",
        "#  Loss function (Weighted for 3 classes)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "#  Optimizer — trains only the unfrozen parameters (the classifier)\n",
        "# Ensure 'model' is the one from phase 1 (where backbone is frozen)\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "#  Mixed-precision scaler\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "NUM_EPOCHS = 4\n",
        "\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # ---- TRAINING ----\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # tqdm gives us a nice progress bar\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
        "\n",
        "    for imgs, labels in loop:\n",
        "        imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (with automatic mixed precision)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Metrics\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # ---- VALIDATION ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_vpreds, all_vlabels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            vpreds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_vpreds.extend(vpreds)\n",
        "            all_vlabels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = accuracy_score(all_vlabels, all_vpreds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilfaQhHjXvyL",
        "outputId": "40d67926-bc07-436a-cfca-a8dc945fd072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Class Weights: tensor([1.7800, 0.6200, 1.2000], device='cuda:0')\n",
            "Starting training for 4 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4 [Train]: 100%|██████████| 451/451 [02:08<00:00,  3.51it/s, loss=0.382]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4: Train Loss: 0.6074 | Train Acc: 0.8251 | Val Loss: 0.2409 | Val Acc: 0.9545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4 [Train]: 100%|██████████| 451/451 [02:07<00:00,  3.53it/s, loss=0.313]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/4: Train Loss: 0.3156 | Train Acc: 0.9254 | Val Loss: 0.1408 | Val Acc: 0.9545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4 [Train]: 100%|██████████| 451/451 [02:08<00:00,  3.50it/s, loss=0.283]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/4: Train Loss: 0.2436 | Train Acc: 0.9268 | Val Loss: 0.1069 | Val Acc: 0.9545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4 [Train]: 100%|██████████| 451/451 [02:06<00:00,  3.56it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4: Train Loss: 0.2155 | Train Acc: 0.9285 | Val Loss: 0.0945 | Val Acc: 0.9545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect true labels, hard predictions, AND probabilities for AUC\n",
        "y_true, y_pred, y_probs = [], [], []\n",
        "model.eval()\n",
        "\n",
        "print(\"Starting evaluation...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader):\n",
        "        imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "        # Use autocast for consistency during inference\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(imgs)\n",
        "\n",
        "        outputs=outputs.float()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        # CHANGED: Calculate Softmax probabilities for ALL 3 classes\n",
        "        # We need the full shape [Batch_Size, 3] for multi-class evaluation\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_probs.extend(probs.cpu().numpy()) # Store probabilities for AUC\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_probs = np.array(y_probs)\n",
        "\n",
        "# --- REPORTING ---\n",
        "print(\"\\nClassification Report:\")\n",
        "# class_names should be ['NORMAL', 'PNEUMONIA', 'UNKNOWN']\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# --- MULTI-CLASS AUC ---\n",
        "try:\n",
        "    # We use 'ovr' (One-vs-Rest) to calculate AUC for 3 classes\n",
        "    auc_score = roc_auc_score(y_true, y_probs, multi_class='ovr', average='macro')\n",
        "    print(f\"Multi-class AUC (Macro OvR): {auc_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYqM2dabdbLa",
        "outputId": "a769881a-1417-41a2-c7c1-eb3a4b9c1d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:15<00:00,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.64      0.93      0.76       234\n",
            "   PNEUMONIA       0.94      0.68      0.79       390\n",
            "     UNKNOWN       1.00      1.00      1.00       600\n",
            "\n",
            "    accuracy                           0.89      1224\n",
            "   macro avg       0.86      0.87      0.85      1224\n",
            "weighted avg       0.91      0.89      0.89      1224\n",
            "\n",
            "Multi-class AUC (Macro OvR): 0.9826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------\n",
        "# PHASE 2 SETUP (Transition3 + DenseBlock4)\n",
        "---------------------------------"
      ],
      "metadata": {
        "id": "W1mh7sm1HLVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "print(\"Starting Targeted Fine-Tuning Phase...\")\n",
        "\n",
        "# 1. RE-FREEZE THE ENTIRE MODEL (Safety step)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. UNFREEZE THE CLASSIFIER (Must be trainable)\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. UNFREEZE TARGETED LAYERS (Transition3 + DenseBlock4)\n",
        "print(\"Unfreezing transition3, denseblock4, and classifier.\")\n",
        "\n",
        "for param in model.features.transition3.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.features.denseblock4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 4. DEFINE OPTIMIZER\n",
        "# Uses a smaller learning rate (1e-5) to preserve pre-trained features\n",
        "FINE_TUNE_LR = 1e-5\n",
        "fine_tune_optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=FINE_TUNE_LR\n",
        ")\n",
        "\n",
        "# 5. CONFIRMATION\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"✅ Phase 2 Configured. Total trainable parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLpEfyLKh2pz",
        "outputId": "8711e091-de4c-424d-ad82-89862f7a50fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Targeted Fine-Tuning Phase...\n",
            "Unfreezing transition3, denseblock4, and classifier.\n",
            "✅ Phase 2 Configured. Total trainable parameters: 2,687,491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 2: PHASE 2 TRAINING ---\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Configuration\n",
        "PNEUMONIA_CLASSIFIER_WEIGHTS_PATH = 'best_pneumonia_classifier.pth'\n",
        "best_val_loss = float('inf')\n",
        "FINE_TUNE_EPOCHS = 20\n",
        "\n",
        "# 1. WEIGHTS (Recalculated for Unknown = 2000 samples)\n",
        "# Normal: 1.78, Pneumonia: 0.62, Unknown: 1.20\n",
        "class_weights = torch.tensor([1.78, 0.62, 1.20], dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "# 2. LOSS & SCALER\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "print(f\"Starting Fine-Tuning for {FINE_TUNE_EPOCHS} epochs...\")\n",
        "\n",
        "for epoch in range(FINE_TUNE_EPOCHS):\n",
        "    # ---- TRAINING ----\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # Fixed: Loop over 'train_loader', not 'train_loader.dataset'\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{FINE_TUNE_EPOCHS}\")\n",
        "\n",
        "    for imgs, labels in loop:\n",
        "        imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "        fine_tune_optimizer.zero_grad()\n",
        "\n",
        "        # Mixed Precision Forward Pass\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward Pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(fine_tune_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Metrics accumulation\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Normalize loss by total images\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # ---- VALIDATION ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_vpreds, all_vlabels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            vpreds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_vpreds.extend(vpreds)\n",
        "            all_vlabels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = accuracy_score(all_vlabels, all_vpreds)\n",
        "\n",
        "    # ---- CHECKPOINTING ----\n",
        "    if val_loss < best_val_loss:\n",
        "        print(f\"\\n--> Val Loss improved ({best_val_loss:.4f} -> {val_loss:.4f}). Saving model.\")\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), PNEUMONIA_CLASSIFIER_WEIGHTS_PATH)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGm3Wyh5h2hx",
        "outputId": "b4fb343e-58cd-42c2-f743-758c52686a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Fine-Tuning for 20 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 451/451 [02:09<00:00,  3.47it/s, loss=0.0446]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (inf -> 0.0585). Saving model.\n",
            "Epoch 1: Train Loss: 0.0709 Acc: 0.9719 | Val Loss: 0.0585 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 451/451 [02:13<00:00,  3.39it/s, loss=0.0548]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.0657 Acc: 0.9739 | Val Loss: 0.0740 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 451/451 [02:11<00:00,  3.42it/s, loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.0599 Acc: 0.9770 | Val Loss: 0.0750 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 451/451 [02:11<00:00,  3.43it/s, loss=0.00706]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.0561 Acc: 0.9785 | Val Loss: 0.0695 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 451/451 [02:08<00:00,  3.50it/s, loss=0.134]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.0524 Acc: 0.9782 | Val Loss: 0.0635 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 451/451 [02:09<00:00,  3.48it/s, loss=0.00312]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (0.0585 -> 0.0532). Saving model.\n",
            "Epoch 6: Train Loss: 0.0490 Acc: 0.9796 | Val Loss: 0.0532 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 451/451 [02:10<00:00,  3.44it/s, loss=0.0059]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss: 0.0533 Acc: 0.9800 | Val Loss: 0.0541 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 451/451 [02:12<00:00,  3.42it/s, loss=0.178]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss: 0.0449 Acc: 0.9817 | Val Loss: 0.0659 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 451/451 [02:12<00:00,  3.40it/s, loss=0.0193]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (0.0532 -> 0.0453). Saving model.\n",
            "Epoch 9: Train Loss: 0.0443 Acc: 0.9831 | Val Loss: 0.0453 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 451/451 [02:11<00:00,  3.42it/s, loss=0.0365]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss: 0.0420 Acc: 0.9846 | Val Loss: 0.0731 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 451/451 [02:11<00:00,  3.42it/s, loss=0.0125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss: 0.0378 Acc: 0.9850 | Val Loss: 0.0734 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 451/451 [02:11<00:00,  3.43it/s, loss=0.0212]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train Loss: 0.0317 Acc: 0.9877 | Val Loss: 0.0461 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 451/451 [02:10<00:00,  3.45it/s, loss=0.00199]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train Loss: 0.0360 Acc: 0.9863 | Val Loss: 0.0575 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 451/451 [02:12<00:00,  3.40it/s, loss=0.00584]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Loss: 0.0332 Acc: 0.9878 | Val Loss: 0.0583 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 451/451 [02:11<00:00,  3.44it/s, loss=0.0138]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train Loss: 0.0366 Acc: 0.9877 | Val Loss: 0.0545 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 451/451 [02:10<00:00,  3.46it/s, loss=0.0166]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (0.0453 -> 0.0397). Saving model.\n",
            "Epoch 16: Train Loss: 0.0323 Acc: 0.9881 | Val Loss: 0.0397 Acc: 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 451/451 [02:13<00:00,  3.37it/s, loss=0.00277]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (0.0397 -> 0.0280). Saving model.\n",
            "Epoch 17: Train Loss: 0.0314 Acc: 0.9875 | Val Loss: 0.0280 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 451/451 [02:11<00:00,  3.43it/s, loss=0.00711]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (0.0280 -> 0.0252). Saving model.\n",
            "Epoch 18: Train Loss: 0.0294 Acc: 0.9885 | Val Loss: 0.0252 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 451/451 [02:10<00:00,  3.45it/s, loss=0.00105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Val Loss improved (0.0252 -> 0.0216). Saving model.\n",
            "Epoch 19: Train Loss: 0.0242 Acc: 0.9922 | Val Loss: 0.0216 Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 451/451 [02:10<00:00,  3.46it/s, loss=0.343]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss: 0.0240 Acc: 0.9917 | Val Loss: 0.0445 Acc: 0.9848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Path where you saved the best model after Phase 2\n",
        "PNEUMONIA_CLASSIFIER_WEIGHTS_PATH = 'best_pneumonia_classifier.pth'\n",
        "# 1. Load the best saved weights into the current model instance\n",
        "print(f\"Loading weights from {PNEUMONIA_CLASSIFIER_WEIGHTS_PATH}...\")\n",
        "try:\n",
        "    # Ensure the model is ready to receive weights\n",
        "    # Note: The model structure (3 classes) must match the saved weights\n",
        "    model.load_state_dict(torch.load(PNEUMONIA_CLASSIFIER_WEIGHTS_PATH))\n",
        "    print(\" Successfully loaded best weights for final testing.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\" Error: Weights file not found at {PNEUMONIA_CLASSIFIER_WEIGHTS_PATH}. Check Phase 2 execution.\")\n",
        "except Exception as e:\n",
        "    print(f\" An error occurred while loading weights: {e}\")\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "# Cell 14: Testing Loop\n",
        "print(\"Starting Final Evaluation on Test Set...\")\n",
        "\n",
        "y_true, y_pred, y_probs = [], [], []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader):\n",
        "        imgs, labels = imgs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "\n",
        "        # Use autocast for consistency during inference\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(imgs)\n",
        "\n",
        "        # --- CRITICAL FIX FOR MIXED PRECISION ---\n",
        "        # Convert to float32 before Softmax to ensure sums = 1.0\n",
        "        outputs = outputs.float()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        # --- CRITICAL CHANGE FOR 3 CLASSES ---\n",
        "        # OLD: probs = torch.softmax(outputs, dim=1)[:, 1]  <-- Binary only\n",
        "        # NEW: Capture probabilities for ALL 3 classes for Multi-class AUC\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_probs = np.array(y_probs)\n",
        "\n",
        "# --- SAFETY NORMALIZATION ---\n",
        "# Ensure probabilities sum to exactly 1.0 to satisfy sklearn strict checks\n",
        "y_probs = y_probs / y_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "# --- REPORTING ---\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"FINAL TEST RESULTS\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Classification Report\n",
        "# class_names should be ['NORMAL', 'PNEUMONIA', 'UNKNOWN']\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Multi-Class AUC Calculation\n",
        "try:\n",
        "    # Calculate Macro Average AUC (One-vs-Rest)\n",
        "    auc_score = roc_auc_score(y_true, y_probs, multi_class='ovr', average='macro')\n",
        "    print(f\"Multi-class AUC (Macro OvR): {auc_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not calculate AUC: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0czKdEQptR-k",
        "outputId": "067f9d72-df1c-47dc-ee4c-35b079dd6335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from best_pneumonia_classifier.pth...\n",
            "✅ Successfully loaded best weights for final testing.\n",
            "Starting Final Evaluation on Test Set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:15<00:00,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "FINAL TEST RESULTS\n",
            "==============================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.92      0.93      0.92       234\n",
            "   PNEUMONIA       0.96      0.95      0.95       390\n",
            "     UNKNOWN       1.00      1.00      1.00       600\n",
            "\n",
            "    accuracy                           0.97      1224\n",
            "   macro avg       0.96      0.96      0.96      1224\n",
            "weighted avg       0.97      0.97      0.97      1224\n",
            "\n",
            "Multi-class AUC (Macro OvR): 0.9953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------\n",
        "#test the model\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "V471Udl5Hp-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "IMG_SIZE = 224\n",
        "class_names = [\"NORMAL\", \"PNEUMONIA\", \"UNKNOWN\"]\n",
        "\n",
        "# # Define the exact preprocessing used during training\n",
        "# # We use 'val_transforms' logic (No random augmentations)\n",
        "# val_transforms = transforms.Compose([\n",
        "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "def predict_single_image(image_path, model, transform):\n",
        "    \"\"\"Loads an image, preprocesses it, and gets the model's prediction.\"\"\"\n",
        "\n",
        "    # 1. Load the image\n",
        "    # IMPORTANT: Convert to RGB.\n",
        "    # - X-rays will become 3-channel Grayscale (r=g=b).\n",
        "    # - Unknown images (CIFAR) will keep their color.\n",
        "    # This difference (Color vs No Color) helps the model detect 'Unknown' images.\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        return None, 0.0, None\n",
        "\n",
        "    # 2. Preprocess the image\n",
        "    input_tensor = transform(img)\n",
        "\n",
        "    # 3. Add a batch dimension and move to GPU\n",
        "    # The model expects a batch of images (B, C, H, W), so we add B=1\n",
        "    input_batch = input_tensor.unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "    # 4. Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use autocast if the model was trained with mixed precision\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            output = model(input_batch)\n",
        "\n",
        "        # Get probabilities using Softmax\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "    # Get the predicted class index\n",
        "    predicted_index = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Get the confidence for the predicted class\n",
        "    confidence = probabilities[0, predicted_index].item()\n",
        "\n",
        "    return predicted_index, confidence, probabilities.cpu().squeeze()\n",
        "\n",
        "# --- USAGE ---\n",
        "# IMPORTANT: Replace this path with the image you want to test\n",
        "NEW_IMAGE_PATH = \"/content/pppennnn.jpg\"\n",
        "\n",
        "print(f\"--- Analysis for: {NEW_IMAGE_PATH.split('/')[-1]} ---\")\n",
        "\n",
        "# Run prediction\n",
        "idx, conf, probs = predict_single_image(NEW_IMAGE_PATH, model, val_transforms)\n",
        "\n",
        "if idx is not None:\n",
        "    predicted_class = class_names[idx]\n",
        "\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    print(f\"Confidence:      {conf:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Detailed Probabilities:\")\n",
        "    print(f\"  Normal:    {probs[0]:.4f}\")\n",
        "    print(f\"  Pneumonia: {probs[1]:.4f}\")\n",
        "    print(f\"  Unknown:   {probs[2]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr9VA2mK9CN8",
        "outputId": "eeafa2d6-ab47-4075-d525-eef553697a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Analysis for: pppennnn.jpg ---\n",
            "Predicted Class: PNEUMONIA\n",
            "Confidence:      0.9751\n",
            "------------------------------\n",
            "Detailed Probabilities:\n",
            "  Normal:    0.0032\n",
            "  Pneumonia: 0.9751\n",
            "  Unknown:   0.0216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------\n",
        "#saving the weight of the model to drive\n",
        "-------------------------------------"
      ],
      "metadata": {
        "id": "xDUfi9lMH0LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# 2. Define paths\n",
        "# The file currently saved in your Colab temporary session\n",
        "LOCAL_MODEL_PATH = 'best_pneumonia_classifier.pth'\n",
        "\n",
        "# The folder in your Google Drive where you want to keep it\n",
        "# It will create a folder named 'My_Medical_Project_Models' in your Drive\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/My_Medical_Project_Models'\n",
        "DRIVE_MODEL_PATH = os.path.join(DRIVE_FOLDER, 'densenet_pneumonia_final.pth')\n",
        "\n",
        "# 3. Create the folder in Drive if it doesn't exist\n",
        "if not os.path.exists(DRIVE_FOLDER):\n",
        "    os.makedirs(DRIVE_FOLDER)\n",
        "    print(f\"Created folder: {DRIVE_FOLDER}\")\n",
        "\n",
        "# 4. Copy the file\n",
        "if os.path.exists(LOCAL_MODEL_PATH):\n",
        "    print(f\"Copying {LOCAL_MODEL_PATH} to Google Drive...\")\n",
        "    shutil.copy(LOCAL_MODEL_PATH, DRIVE_MODEL_PATH)\n",
        "\n",
        "    if os.path.exists(DRIVE_MODEL_PATH):\n",
        "        print(\"-\" * 40)\n",
        "        print(f\" SUCCESS! Model saved permanently at:\")\n",
        "        print(f\"{DRIVE_MODEL_PATH}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"You can now close Colab safely. The model is in your Drive.\")\n",
        "    else:\n",
        "        print(\" Error: File copy failed.\")\n",
        "else:\n",
        "    print(f\" Error: Could not find {LOCAL_MODEL_PATH}. Did you run the training cell?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhJQJQRYWssA",
        "outputId": "e8a9823f-00fe-4ba2-eae0-7c6778d376c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created folder: /content/drive/MyDrive/My_Medical_Project_Models\n",
            "Copying best_pneumonia_classifier.pth to Google Drive...\n",
            "----------------------------------------\n",
            "✅ SUCCESS! Model saved permanently at:\n",
            "/content/drive/MyDrive/My_Medical_Project_Models/densenet_pneumonia_final.pth\n",
            "----------------------------------------\n",
            "You can now close Colab safely. The model is in your Drive.\n"
          ]
        }
      ]
    }
  ]
}